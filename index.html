<!DOCTYPE HTML>
<html>

    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="keywords" content="xirl, self-supervised learning, imitation learning, reinforcement learning, robotics, neural networks, machine learning">
        <title>XIRL: Cross-embodiment Inverse Reinforcement Learning</title>

        <!--Open Graph-->
        <meta property="og:title" content="XIRL"/>
        <meta property="og:type" content="website"/>
        <meta property="og:url" content="http://x-irl.github.io/"/>
        <meta property="og:description" content="Cross-embodiment visual imitation via learned reward functions."/>
        <meta property="og:image" content="https://x-irl.github.io/images/teaser.png"/>

        <!--Twitter Card-->
        <meta name="twitter:card" content="summary_large_image"/>
        <meta name="twitter:title" content="XIRL"/>
        <meta name="twitter:image" content="https://x-irl.github.io/images/teaser.png"/>
        <meta name="twitter:url" content="http://x-irl.github.io/"/>
        <meta name="twitter:description" content="Cross-embodiment visual imitation via learned reward functions."/>

        <!-- CSS -->
        <link rel="stylesheet" href="main.css" />

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-146254085-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-146254085-1');
        </script>
    </head>

    <body id="top">
        <div id="main" style="padding-bottom:1em; padding-top: 2em; width: 60em; max-width: 70em; margin-left: auto; margin-right: auto;">
            <section id="four">
                <!-- Title -->
                <h1 style="text-align: center; margin-bottom: 3px; padding-top: 100px;"><font color="4e79a7">XIRL:</font> Cross-embodiment Inverse Reinforcement Learning</h1>
                <h3 style="text-align: center;margin-bottom: 25px;">Conference on Robot Learning (CoRL) 2021</h3>
                <div style="clear:both;"></div>

                <!-- Buttons -->
                <div style="text-align:center;">
                    <a href="https://ai.googleblog.com/2022/02/robot-see-robot-do.html" class="button">Blog</a>
                    <a href="https://arxiv.org/abs/2106.03911" class="button">Paper</a>
                    <a href="https://github.com/google-research/google-research/tree/master/xirl" class="button">Code</a>
                    <a href="https://github.com/kevinzakka/x-magical" class="button">Benchmark</a>
                    <a href="images/poster.png" class="button">Poster</a>
                </div>
                <br>

                <p align="justify"><b>Abstract</b>. We investigate the visual <font color="4e79a7">cross-embodiment</font> imitation setting, in which agents learn policies from videos of other agents (such as humans) demonstrating the same task, but with stark differences in their <font color="4e79a7">embodiments</font> -- shape, actions, end-effector dynamics, etc. In this work, we demonstrate that it is possible to automatically discover and learn vision-based reward functions from cross-embodiment demonstration videos that are robust to these differences. Specifically, we present a self-supervised method for Cross-embodiment Inverse Reinforcement Learning (XIRL) that leverages <font color="4e79a7">temporal cycle-consistency</font> constraints to learn deep visual embeddings that capture task progression from offline videos of demonstrations across multiple expert agents, each performing the same task differently due to embodiment differences. Prior to our work, producing rewards from self-supervised embeddings typically required alignment with a reference trajectory, which may be difficult to acquire under stark embodiment differences. We show empirically that if the embeddings are aware of <font color="4e79a7">task progress</font>, simply taking the negative distance between the current state and goal state in the learned embedding space is useful as a reward for training policies with <font color="4e79a7">reinforcement learning</font>. We find our learned reward function not only works for embodiments seen during training, but also generalizes to entirely new embodiments. Additionally, when transferring real-world human demonstrations to a simulated robot, we find that XIRL is more sample efficient than current best methods.</p>

                <hr/>
                <h3>Method</h3>
                <div style="text-align: center; margin-top: 1em;"><img src="./images/teaser.gif" style="width: 75%; padding-bottom: 1em;"></div>

                <!----------------------------------------------------------------->
                <hr/>
                <h3>Paper</h3>

                <p style="margin-top: -1em; margin-bottom: 0em;"><font color="4e79a7">&#9733; Best Paper Award Finalist &#9733;</font></p>
                <p>Conference on Robot Learning (CoRL) 2021</p>
                <p style="margin-bottom: 0em;">Latest version (Sept. 2021): <a href="https://arxiv.org/abs/2106.03911">arXiv:2106.03911 [cs.RO]</a></p>
                <p style="margin-bottom: 1em;">OpenReview submission: <a href="https://openreview.net/forum?id=RO4DM85Z4P7">here</a>
                <div class="12u$"><a href="https://arxiv.org/abs/2106.03911"><span class="image fit" style="border: 1px solid; border-color: #888888;"><img src="images/paper-thumbnail.png"/></span></a></div>

                <!-- <hr/>
                <div class="row">
                    <h3>Video Summary</h3>
                    <div class="12u$ 12u$(xsmall)" style="text-align: center;">
                        Coming soon...
                    </div>
                </div> -->

                <hr>
                <div class="row" style="margin-bottom: -30px;">
                    <div class="6u 12u$(xsmall)">
                        <div style="display: block; padding-bottom: 20px;">
                            <image src="images/github.png" height="30px" style="vertical-align: middle;"/>
                            <span style="vertical-align: middle; padding-left: 10px; font-size: 15pt;">Code</span>
                        </div>
                        <p style="padding-bottom: 10px;">The official code is available on Github. It includes:</p>
                        <ul>
                            <li><a href="https://github.com/google-research/google-research/tree/master/xirl">Self-supervised pretraining and policy learning.</a></li>
                            <li><a href="https://github.com/kevinzakka/x-magical">The standalone simulated X-MAGICAL benchmark.</a></li>
                        </ul>
                    </div>
                    <div class="6u$ 12u$(xsmall)">
                        <div style="display: block; padding-bottom: 20px; padding-left: 0px;">
                            <image src="images/database_icon.svg" height="30px" style="vertical-align: middle;"/>
                            <span style="vertical-align: middle; padding-left: 10px; font-size: 15pt;">Datasets</span>
                        </div>

                        <p style="padding-bottom: 10px;">We're also releasing 2 datasets:</p>
                        <ul>
                            <li><a href="https://drive.google.com/file/d/1VdMRYu0Y-ep_vq28hW2n0UZaow2iaW1i/view?usp=sharing">The simulated X-MAGICAL expert demonstration dataset.</a></li>
                            <li><a href="">The real-world X-REAL cross-embodiment dataset.</a></li>
                        </ul>
                    </div>
                </div>

                <!----------------------------------------------------------------->
                <hr>
                <h3>Team</h3>
                <section>
                    <div class="people-profile"><a href="https://kzakka.com/"><img src="images/kevin-thumbnail.jpg"><p>Kevin Zakka<sup>1,3</sup></p></a></div>
                    <div class="people-profile"><a href="https://andyzeng.github.io/"><img src="images/andy.jpg"><p>Andy Zeng<sup>1</sup></p></a></div>
                    <div class="people-profile"><a href="http://www.peteflorence.com/"><img src="images/pete.png"><p>Pete Florence<sup>1</sup></p></a></div>
                    <div class="people-profile"><a href="https://jonathantompson.github.io/"><img src="images/jonathan.jpeg"><p>Jonathan Tompson<sup>1</sup></p></a></div>
                    <div class="people-profile"><a href="https://web.stanford.edu/~bohg/"><img src="images/jeannette.png"><p>Jeannette Bohg<sup>2</sup></p></a></div>
                    <div class="people-profile"><a href="https://debidatta.github.io/"><img src="images/debi.jpg"><p>Debidatta Dwibedi<sup>1</sup></p></a></div>
                    <div style="clear: both;"></div>
                </section>
                <p style="font-size: 12pt;margin-top: 2em;margin-bottom: -20px; text-align: center;"><sup>1</sup> Robotics at Google&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup> Stanford University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup></sup><sup>3</sup> UC Berkeley&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup></sup></p>

                <!----------------------------------------------------------------->
                <hr/>
                <h3>X-MAGICAL Benchmark</h3>
                <p>We introduce a benchmark extension of MAGICAL, called <a href="https://github.com/kevinzakka/x-magical">x-MAGICAL</a>, specifically geared towards cross-embodiment imitation. The goal of X-MAGICAL is to test how well imitation or reward learning techniques can adapt to systematic embodiment gaps between the demonstrator and the learner. For example, in the Sweeping task, some agents can sweep all debris in one motion while others need to sweep them one at a time. These differences in execution speeds and state-action trajectories pose challenges for current LfD techniques, and the ability to generalize across embodiments is precisely what this benchmark evaluates.</p>

                <div style="text-align: center; margin-top: 2em;">
                    <div style="display: inline-block">
                        <image src="images/xmagical/gripper-sweep.gif" height="150px">
                        <br/>
                        <strong>Gripper</strong>
                    </div>
                    <div style="display: inline-block">
                        <image src="images/xmagical/longstick-sweep.gif" height="150px">
                        <br />
                        <strong>Longstick</strong>
                    </a>
                    </div>
                    <div style="display: inline-block">
                        <image src="images/xmagical/mediumstick-sweep.gif" height="150px">
                        <br />
                        <strong>Mediumstick</strong>
                    </a>
                    </div>
                    <div style="display: inline-block">
                        <image src="images/xmagical/shortstick-sweep.gif" height="150px">
                        <br />
                        <strong>Shortstick</strong>
                    </a>
                    </div>
                </div>
                <br>

                <hr/>
                <h3>X-REAL Dataset</h3>
                <p style="margin-bottom: 15px;">
                    We collect a real-world dataset named <font color="4e79a7">X-REAL</font> (Cross-embodiment Real-world demonstrations), which contains 93 demonstration videos of different embodiments (manifested as different manipulator end-effectors) solving the same manipulation task in the real-world: <i>transferring five pens to two cups consecutively</i>. This is a multi-step manipulation task where the pens on the table need to be lifted to one cup and then moved again to a separate cup. The different end-effectors consist in a human hand as well as 6 tools purchased from Amazon. We showcase some of the embodiments below.
                </p>

                <div class="box alt">
                    <div class="row">
                        <div class="4u"><video class="image fit" style="margin-bottom: 0.5em;" controls><source src="./images/videos/one_hand_five_fingers.MP4" type="video/mp4">Your browser does not support this video.</video><h5 style="color: #a2a2a2; margin-bottom: 1em; text-align: center;">One Hand Five Fingers</h5></div>
                        <div class="4u"><video class="image fit" style="margin-bottom: 0.5em;" controls><source src="./images/videos/one_hand_two_fingers.MP4" type="video/mp4">Your browser does not support this video.</video><h5 style="color: #a2a2a2; margin-bottom: 1em; text-align: center;">One Hand Two Fingers</h5></div>
                        <div class="4u"><video class="image fit" style="margin-bottom: 0.5em;" controls><source src="./images/videos/two_hands_two_fingers.MP4" type="video/mp4">Your browser does not support this video.</video><h5 style="color: #a2a2a2; margin-bottom: 1em; text-align: center;">Two Hands Two Fingers</h5></div>
                        <div class="4u"><video class="image fit" style="margin-bottom: 0.5em;" controls><source src="./images/videos/rms.MP4" type="video/mp4">Your browser does not support this video.</video><h5 style="color: #a2a2a2; margin-bottom: 1em; text-align: center;">RMS Grabber</h5></div>
                        <div class="4u"><video class="image fit" style="margin-bottom: 0.5em;" controls><source src="./images/videos/tongs.MP4" type="video/mp4">Your browser does not support this video.</video><h5 style="color: #a2a2a2; margin-bottom: 1em; text-align: center;">Tongs</h5></div>
                        <div class="4u"><video class="image fit" style="margin-bottom: 0.5em;" controls><source src="./images/videos/quick_grip.MP4" type="video/mp4">Your browser does not support this video.</video><h5 style="color: #a2a2a2; margin-bottom: 1em; text-align: center;">Irwin Quick-Grip</h5></div>
                    </div>
                </div>

                <hr/>
                <h3>Qualitative Results</h3>

                <h4>XIRL Reward Visualization</h4>
                <div class="box alt">
                    <div class="row 50% uniform" style="text-align: center">
                      <div class="4u"><img src="images/rlv_learned_positive.gif" style="max-width:75%;"><h5>Positive demo</h5></div>
                      <div class="4u"><img src="images/rlv_learned_negative.gif" style="max-width:75%;"><h5>Negative demo</h5></div>
                      <div class="4u$"><img src="images/rlv_learned_overshoot.gif" style="max-width:75%;"><h5>Overshoot demo</h5></div>
                    </div>
                </div>

                <div class="box alt">
                    <div class="row 50% uniform" style="text-align: center">
                      <div class="6u"><img src="images/gripper_pos.gif" style="max-width:100%;"><h5>Positive demo</h5></div>
                      <div class="6u$"><img src="images/gripper_neg.gif" style="max-width:100%;"><h5>Negative demo</h5></div>
                    </div>
                </div>
                <br />


                <h4>t-SNE Projection</h4>
                <p style="padding-bottom: 15px;">
                    We visualize the t-SNE projection of four demonstrations in simulation on an X-MAGICAL embodiment, and in the real-world. In each video below, the frame border color corresponds to the trajectory color in the middle plot and the highlighted marker denotes the current frameâ€™s embedding.
                </p>

                <div class="box alt">
                    <div class="row 50% uniform" style="text-align: center">
                      <div class="6u"><video class="image fit" controls style="margin-bottom: 10px;"><source src="./images/videos/tsne_xirl.mp4" type="video/mp4">Your browser does not support this video.</video></div>
                      <div class="6u$"><video class="image fit" controls style="margin-bottom: 10px;"><source src="./images/videos/tsne_rlv.mp4" type="video/mp4">Your browser does not support this video.</video></div>
                    </div>
                  </div>

                <hr/>
                <h3>Citation</h3>
                <div class="code">
<pre>
@article{zakka2021xirl,
    title = {XIRL: Cross-embodiment Inverse Reinforcement Learning},
    author = {Zakka, Kevin and Zeng, Andy and Florence, Pete and Tompson, Jonathan and Bohg, Jeannette and Dwibedi, Debidatta},
    journal = {Conference on Robot Learning (CoRL)},
    year = {2021}
}</pre>
                </div>

                <hr/ style="margin-bottom: 0.5em">
                <h3>Acknowledgements</h3>
                    <p>We would like to thank Alex Nichol, Nick Hynes, Sean Kirmani, Brent Yi and Jimmy Wu for fruitful technical discussions, Sam Toyer for invaluable help with setting up the simulated benchmark, and Karl Schmeckpeper for discussions and help related to RLV.</p>

                <div style="text-align: center;">
                <p style="font-size: 15px; margin-top: 30px;">If you have any questions, please feel free to contact <a href="https://kzakka.com/">Kevin Zakka</a>.</p>
                </div>

            </section>
        </div>

        <!-- Scripts -->
            <script src="assets/js/jquery.min.js"></script>
            <script src="assets/js/jquery.poptrox.min.js"></script>
            <script src="assets/js/skel.min.js"></script>
            <script src="assets/js/util.js"></script>
            <script src="assets/js/main.js"></script>
    </body>
</html>
